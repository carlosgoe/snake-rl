{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pg import PG\n",
    "from dqn import DQN\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math\n",
    "from time import sleep\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Snake:\n",
    "\n",
    "    def __init__(self, size=(16, 16), gui=False):\n",
    "        self.size = size\n",
    "        self.gui = gui\n",
    "        self.snake = [(size[0] // 2, 3), (size[0] // 2, 2), (size[0] // 2, 1)]\n",
    "        self.positions = [(x, y) for x in range(size[0]) for y in range(size[1])]\n",
    "        self.direction = (0, 1)\n",
    "        self.food = random.choice([p for p in self.positions if p not in self.snake])\n",
    "        self.points = 0\n",
    "        self.game_over = False\n",
    "        self.visualize()\n",
    "\n",
    "    def visualize(self):\n",
    "        if self.gui:\n",
    "            field = np.zeros(self.size)\n",
    "            field[self.food] = 3\n",
    "            field[np.array(self.snake)[:, 0], np.array(self.snake)[:, 1]] = 1\n",
    "            field[self.snake[0]] = 2\n",
    "            replacements = {'2.': '◈', '0.': '▢', '1.': '▩', '3.': '◉', '[': '', ']': ''}\n",
    "            output = str(field)\n",
    "            for key, val in replacements.items():\n",
    "                output = output.replace(key, val)\n",
    "            clear_output(wait=True)\n",
    "            print('Points: {0}\\n\\n {1}'.format(self.points, output))\n",
    "            sleep(0.1)\n",
    "            if self.game_over:\n",
    "                sleep(1)\n",
    "            else:\n",
    "                sleep(0.1)\n",
    "                \n",
    "    def get_vector(self, direct):\n",
    "        # Vectors to select from\n",
    "        vecs = [(0, 1), (1, 0), (0, -1), (-1, 0)]\n",
    "        # Right\n",
    "        if direct == 0:\n",
    "            return vecs[(vecs.index(self.direction) + 1) % 4]\n",
    "        # Left\n",
    "        if direct == 1:\n",
    "            return vecs[(vecs.index(self.direction) - 1) % 4]\n",
    "        # Forward\n",
    "        return self.direction\n",
    "\n",
    "    def obs_and_invalid(self):\n",
    "        # Get angle to food\n",
    "        dir_food = tuple(np.array(self.food) - np.array(self.snake[0]))\n",
    "        angle = math.acos((self.direction[0] * dir_food[0] + self.direction[1] * dir_food[1]) / math.sqrt(dir_food[0] ** 2 + dir_food[1] ** 2)) / math.pi\n",
    "        # Get positions right, left, and in front of snake\n",
    "        positions = [tuple(np.array(self.snake[0]) + np.array(self.get_vector(i))) for i in range(3)]\n",
    "        # Get booleans that indicate whether there is an obstacle right, left, and in front of snake\n",
    "        obstacles = [float(p not in self.positions or p in self.snake[:-1]) for p in positions]\n",
    "        # Get invalid actions if there are 1 or 2\n",
    "        invalid = np.argwhere(np.array(obstacles) == 1).flatten().tolist() if 0 < np.sum(obstacles) < 3 else []\n",
    "        # Return observation (as numpy array) and invalid actions\n",
    "        return np.array(obstacles + [angle]), invalid\n",
    "\n",
    "    def step(self, action):\n",
    "        # Get new movement direction of snake\n",
    "        vec = self.get_vector(action)\n",
    "        # Calculate new position of snake's head\n",
    "        new_pos = tuple(np.array(self.snake[0]) + np.array(vec))\n",
    "        # Check if new position is within the borders and not part of snake (else: game over and reward -2)\n",
    "        if new_pos not in self.positions or new_pos in self.snake[:-1]:\n",
    "            self.game_over = True\n",
    "            reward = -2\n",
    "        else:\n",
    "            # Calculate the current and the new distance to food\n",
    "            d_food_prev = np.sum(np.abs(np.array(self.food) - np.array(self.snake[0])))\n",
    "            d_food_new = np.sum(np.abs(np.array(self.food) - np.array(new_pos)))\n",
    "            # Apply move to environment\n",
    "            self.snake = [new_pos] + self.snake\n",
    "            last = self.snake[-1]\n",
    "            self.snake = self.snake[:-1]\n",
    "            self.direction = vec\n",
    "            # Check if food is reached and increase length of snake\n",
    "            if new_pos == self.food:\n",
    "                self.snake.append(last)\n",
    "                self.points += 1\n",
    "                self.food = random.choice([p for p in self.positions if p not in self.snake])\n",
    "            # reward of 1 if food is reached or distance to food has decreased, else -1\n",
    "            reward = 1 if d_food_new < d_food_prev else -1\n",
    "        # Visualize environment (only if gui is enabled)\n",
    "        self.visualize()\n",
    "        # Get current observation and invalid actions\n",
    "        obs, invalid = self.obs_and_invalid()\n",
    "        return obs, reward, self.game_over, invalid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural network parameters\n",
    "layers =[4, (32, 'elu'), (32, 'elu'), (3, 'softmax')]\n",
    "loss = keras.losses.categorical_crossentropy\n",
    "optimizer = keras.optimizers.Adam(lr=0.01)\n",
    "# agent parameter\n",
    "discount_factor = 0.95\n",
    "# import from file path, None for freshly initialized model\n",
    "file = None\n",
    "\n",
    "agent = PG(layers, loss, optimizer, discount_factor, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set number of episodes and iterations per episode\n",
    "iterations = 500\n",
    "episodes = 25\n",
    "# set maximum number of steps performed without the score changing\n",
    "max_steps_per_score = 128\n",
    "# define whether to train the agent or not\n",
    "train = True\n",
    "\n",
    "best = None\n",
    "mean_rewards = []\n",
    "for i in range(iterations):\n",
    "    all_rewards = []\n",
    "    all_grads = []\n",
    "    all_points = []\n",
    "    for e in range(episodes):\n",
    "        current_rewards = []\n",
    "        current_grads = []\n",
    "        # Get initial observation\n",
    "        env = Snake(gui=not train)\n",
    "        obs, invalid = env.obs_and_invalid()\n",
    "        # Store steps per score in dictionary\n",
    "        steps_per_score = {}\n",
    "        while steps_per_score.get(env.points, 0) < max_steps_per_score:\n",
    "            # Get action and corresponding gradient\n",
    "            action, grads = agent.run_policy(obs, invalid)\n",
    "            # Perform the action to get new observation and reward data \n",
    "            obs, reward, done, invalid = env.step(action)\n",
    "            # Save reward / gradient in current_rewards / current_gradients\n",
    "            current_rewards.append(reward)\n",
    "            current_grads.append(grads)\n",
    "            # Increase steps of current score by one\n",
    "            steps_per_score[env.points] = steps_per_score.get(env.points, 0) + 1\n",
    "            # Exit loop if game over\n",
    "            if done:\n",
    "                break\n",
    "        # Save lists current_rewards / current_grads in all_rewards / all_grads\n",
    "        all_rewards.append(current_rewards)\n",
    "        all_grads.append(current_grads)\n",
    "        all_points.append(env.points)\n",
    "    mean_reward = np.sum([r for e in all_rewards for r in e]) / episodes\n",
    "    mean_rewards.append(mean_reward)\n",
    "    if train:\n",
    "        print('Iteration {0}/{1} - mean reward, score: {2}, {3}'.format(i + 1, iterations, mean_reward, np.mean(all_points)))\n",
    "        # Save model if it scored best\n",
    "        if best is None or mean_reward >= best:\n",
    "            agent.save('snake_pg')\n",
    "            best = mean_reward\n",
    "            print('Model saved.')\n",
    "        # Use collected reward and gradient data to train agent\n",
    "        agent.apply_grads(all_rewards, all_grads)\n",
    "# Plot mean rewards\n",
    "plt.plot(range(iterations), mean_rewards)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Mean reward')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snake has 4 input values and 3 actions\n",
    "n_obs = 4\n",
    "n_actions = 3\n",
    "# neural network parameters\n",
    "hidden_layers = [(96, 'elu'), (96, 'elu')]\n",
    "optimizer = keras.optimizers.Adam(lr=1e-3)\n",
    "# agent parameters\n",
    "discount_factor = 0.95\n",
    "buffer_size = 50000\n",
    "# import from file path, None for freshly initialized model\n",
    "file = None\n",
    "\n",
    "agent = DQN(n_obs, hidden_layers, n_actions, optimizer, discount_factor, buffer_size, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set number of episodes\n",
    "episodes = 25000\n",
    "# set maximum number of steps performed without the score changing\n",
    "max_steps_per_score = 128\n",
    "# set the first number of episodes in which the agent is not trained\n",
    "n_pretrain = 100\n",
    "# update target model every ... episodes\n",
    "update_target = 200\n",
    "batch_size = 128\n",
    "# set exploration rate decay from ... to ... in ... steps\n",
    "epsilon_decay = 1, 0.01, 15000\n",
    "# define wheter to train the agent or not\n",
    "train = True\n",
    "\n",
    "best = None\n",
    "total_rewards = []\n",
    "scores = []\n",
    "for e in range(episodes):\n",
    "    total_reward = 0\n",
    "    # Initialize environment and get initial state\n",
    "    env = Snake(gui=not train)\n",
    "    state, invalid = env.obs_and_invalid()\n",
    "    # Store steps per score in dictionary\n",
    "    steps_per_score = {}\n",
    "    while steps_per_score.get(env.points, 0) < max_steps_per_score:\n",
    "        # Get agent's action\n",
    "        epsilon = max(epsilon_decay[0] - e / epsilon_decay[2], epsilon_decay[1]) if train else 0\n",
    "        action = agent.play_one_step(state, epsilon, invalid)\n",
    "        # Let environment perform action and update current state\n",
    "        next_state, reward, done, invalid = env.step(action)\n",
    "        agent.add_experience(state, action, reward, next_state, done, invalid)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        # Increase steps of current score by one\n",
    "        steps_per_score[env.points] = steps_per_score.get(env.points, 0) + 1\n",
    "        # Exit loop if game over\n",
    "        if done:\n",
    "            break\n",
    "    # Save and print game data\n",
    "    total_rewards.append(total_reward)\n",
    "    scores.append(env.points)\n",
    "    if train:\n",
    "        print('Episode {0}/{1} - total reward, score: {2}, {3}'.format(e + 1, episodes, total_reward, env.points))\n",
    "        # Save model if the highest reward has been collected\n",
    "        if best is None or total_reward >= best: \n",
    "            agent.save('snake_dqn')\n",
    "            best = total_reward\n",
    "            print('Model saved.')\n",
    "        # Perform training step\n",
    "        if e >= n_pretrain:\n",
    "            agent.training_step(batch_size)\n",
    "            if e % update_target == 0:\n",
    "                agent.update_target_model()\n",
    "# Plot scores and total_rewards\n",
    "plt.plot(range(episodes), total_rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
